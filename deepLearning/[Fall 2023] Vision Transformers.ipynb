{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanhao5103233729/github-colab/blob/main/%5BFall%202023%5D%20Vision%20Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT Assignment\n",
        "Authors: Alexander Wan, Aryan Jain"
      ],
      "metadata": {
        "id": "Tt6WRWv2JXJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Goals\n",
        "\n",
        "\n",
        "1. Familiarity with the Vision Transformer architecture\n",
        "2. Familiarity with the self-attention algorithm\n",
        "3. Practice with PyTorch matrix operations\n",
        "\n"
      ],
      "metadata": {
        "id": "uTUA3I0hT2EM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "1. Implement multi-head self-attention\n",
        "2. Incorporate that into a ViT"
      ],
      "metadata": {
        "id": "QM620ckwWDBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runtime Acceleration\n",
        "Colab limits GPU usage, so set `device` below as `'cpu'` and change your runtime to CPU as well (Runtime > Change runtime type) when you're developing, and only change it to `'cuda'` (and your runtime to GPU) when you're ready to train."
      ],
      "metadata": {
        "id": "_U_6XOlXM61e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "#device = 'cuda'"
      ],
      "metadata": {
        "id": "dbLiRflPIoPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head self-attention\n",
        "Begin by implementing multiheaded self-attention. Do **not** use any `for` loops, and instead put all of the calculations into [batch matrix multiplications](https://pytorch.org/docs/stable/generated/torch.bmm.html) or [Linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
        "\n",
        "Useful references include the lecture slides on transformers and ViTs, and the [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog post.\n",
        "\n",
        "Hint: you are not required to use the exact skeleton code below. Feel free to use `torch.einsum` if you prefer it (this is something you will have to figure out from the PyTorch documentation yourself; this function is somewhat non-intuitive at first but it's extremely powerful once you truly understand how it works!).\n"
      ],
      "metadata": {
        "id": "VCsSGbgWWul-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MSA(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, num_heads):\n",
        "    '''\n",
        "    input_dim: Dimension of input token embeddings\n",
        "    embed_dim: Dimension of internal key, query, and value embeddings\n",
        "    num_heads: Number of self-attention heads\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.K_embed = nn.Linear(input_dim, embed_dim, bias=False)\n",
        "    self.Q_embed = nn.Linear(input_dim, embed_dim, bias=False)\n",
        "    self.V_embed = nn.Linear(input_dim, embed_dim, bias=False)\n",
        "    self.out_embed = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: input of shape (batch_size, max_length, input_dim)\n",
        "    return: output of shape (batch_size, max_length, embed_dim)\n",
        "    '''\n",
        "\n",
        "    batch_size, max_length, given_input_dim = x.shape\n",
        "    assert given_input_dim == self.input_dim\n",
        "    assert max_length % self.num_heads == 0\n",
        "\n",
        "    # You shouldn't need to initialize any new modules. Everything you need is\n",
        "    # already in __init__\n",
        "\n",
        "    # HINT: If you're stuck on how to handle multiple heads without for loops, try to\n",
        "    # reshape matrix such that the batch_size is num_heads * batch_size\n",
        "    # e.g. if you have two heads, you'd be doing self-attention twice per instance\n",
        "    # in the batch, so you essentially have batch_size * 2\n",
        "\n",
        "    # HINT 2: Feel free to reference: https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html\n",
        "    # although make sure you understand what each command does\n",
        "\n",
        "    # this implementation projects KQV before splitting into multiple heads\n",
        "    # but you can also split into multiple heads first\n",
        "\n",
        "    # compute KQV as a whole, embedding and\n",
        "    x = x.reshape(batch_size * max_length, -1)\n",
        "    K = self.K_embed(x).reshape(batch_size, max_length, self.embed_dim) # (batch_size, max_length, embed_dim)\n",
        "    # TODO: Compute Q\n",
        "    # TODO: Compute V\n",
        "\n",
        "    # TODO: split each KQV into heads, by reshaping each into (batch_size, max_length, self.num_heads, indiv_dim)\n",
        "    indiv_dim = self.embed_dim // self.num_heads\n",
        "    K = # TODO\n",
        "    Q = # TODO\n",
        "    V = # TODO\n",
        "\n",
        "    K = K.permute(0, 2, 1, 3) # (batch_size, num_heads, max_length, embed_dim / num_heads)\n",
        "    Q = Q.permute(0, 2, 1, 3)\n",
        "    V = V.permute(0, 2, 1, 3)\n",
        "\n",
        "    K = K.reshape(batch_size * self.num_heads, max_length, indiv_dim)\n",
        "    Q = Q.reshape(batch_size * self.num_heads, max_length, indiv_dim)\n",
        "    V = V.reshape(batch_size * self.num_heads, max_length, indiv_dim)\n",
        "\n",
        "    # transpose and batch matrix multiply\n",
        "    # This is our K transposed so we can do a simple batched matrix multiplication (see torch.bmm for more details and the quick solution)\n",
        "    K_T = K.permute(0, 2, 1)\n",
        "    # TODO: Compute the weights before dividing by square root of d (batch_size * num_heads, max_length, max_length)\n",
        "    QK =\n",
        "\n",
        "    # calculate weights by dividing everything by the square root of d (self.embed_dim)\n",
        "    weights = # TODO\n",
        "    weights = # TODO Take the softmax over the last dimension (see torch.functional.Softmax) (batch_size * num_heads, max_length, max_length)\n",
        "\n",
        "    # TODO get weighted average... see torch.bmm for a one line solution\n",
        "    # weights is (batch_size * num_heads, max_length, max_length) and V is (batch_size * self.num_heads, max_length, indiv_dim)\n",
        "    # so we want the matrix multiplication of weights and V\n",
        "    w_V =\n",
        "\n",
        "    # rejoin heads\n",
        "    w_V = w_V.reshape(batch_size, self.num_heads, max_length, indiv_dim)\n",
        "    w_V = w_V.permute(0, 2, 1, 3) # (batch_size, max_length, num_heads, embed_dim / num_heads)\n",
        "    w_V = w_V.reshape(batch_size, max_length, self.embed_dim)\n",
        "\n",
        "    out = self.out_embed(w_V)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "_q-UREAdT1WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the ViT architecture\n",
        "You will be implementing the ViT architecture based on the \"An image is worth 16x16 words\" paper.\n",
        "\n",
        "Although the ViT and Transformer architecture are very similar, note a few differences:\n",
        "\n",
        "1. Image patches instead of discrete tokens as input.\n",
        "2. [GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) is used for the linear layers in the transformer layer (instead of ReLU)\n",
        "3. LayerNorm before the sublayer instead of after.\n",
        "4. Dropout after every linear layer except for KQV projections and also directly after adding positional embeddings to the patch embeddings.\n",
        "5. Learnable [CLS] token at the beginning of the input.\n",
        "\n",
        "A useful reference is Figure 1 in the [paper](https://arxiv.org/pdf/2010.11929.pdf)."
      ],
      "metadata": {
        "id": "ZoajGRYKsI29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, implement a single layer:"
      ],
      "metadata": {
        "id": "Blpaw39U5Y9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTLayer(nn.Module):\n",
        "  def __init__(self, num_heads, input_dim, embed_dim, mlp_hidden_dim, dropout=0.1):\n",
        "    '''\n",
        "    num_heads: Number of heads for multi-head self-attention\n",
        "    embed_dim: Dimension of internal key, query, and value embeddings\n",
        "    mlp_hidden_dim: Hidden dimension of the linear layer\n",
        "    dropout: Dropout rate\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.msa = MSA(input_dim, embed_dim, num_heads)\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "    self.w_o_dropout = nn.Dropout(dropout)\n",
        "    self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "                              nn.GELU(),\n",
        "                              nn.Dropout(dropout),\n",
        "                              nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "                              nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: input embeddings (batch_size, max_length, input_dim)\n",
        "    return: output embeddings (batch_size, max_length, embed_dim)\n",
        "    '''\n",
        "\n",
        "    # TODO: Fill in the code for the forward pass below\n",
        "    # You shouldn't need to initialize any more modules, everything you need is already\n",
        "    # in __init__\n",
        "    # A forward function consists of:\n",
        "    # 1) LayerNorm of x\n",
        "    # 2) Self-Attention on output of 1)\n",
        "    # 3) Dropout\n",
        "    # 4) Residual w/ original x\n",
        "    # 5) LayerNorm\n",
        "    # 6) MLP\n",
        "    # 7) Residual\n"
      ],
      "metadata": {
        "id": "DffgJiJDsA9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A portion of the full network is already implemented for you. Your task is to implement the preprocessing code, converting raw images into patch embeddings + positional embeddings + dropout, with a learnable CLS token at the beginning of the input.\n",
        "\n",
        "Note that patch embeddings are to be added to positional embeddings elementwise, so the input embedding dimensions is size embed_dim."
      ],
      "metadata": {
        "id": "o8CONmQMfK4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, patch_dim, image_dim, num_layers, num_heads, embed_dim, mlp_hidden_dim, num_classes, dropout):\n",
        "        '''\n",
        "        patch_dim: patch length and width to split image by\n",
        "        image_dim: image length and width\n",
        "        num_layers: number of layers in network\n",
        "        num_heads: number of heads for multi-head attention\n",
        "        embed_dim: dimension to project images patches to and dimension to use for position embeddings\n",
        "        mlp_hidden_dim: hidden dimension of linear layer\n",
        "        num_classes: number of classes to classify in data\n",
        "        dropout: dropout rate\n",
        "        '''\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_dim = patch_dim\n",
        "        self.image_dim = image_dim\n",
        "        self.input_dim = self.patch_dim * self.patch_dim * 3\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.patch_embedding = nn.Linear(self.input_dim, embed_dim)\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, (image_dim // patch_dim) ** 2 + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([])\n",
        "        for i in range(num_layers):\n",
        "            self.encoder_layers.append(ViTLayer(num_heads, embed_dim, embed_dim, mlp_hidden_dim, dropout))\n",
        "\n",
        "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, images):\n",
        "        '''\n",
        "        images: raw image data (batch_size, channels, rows, cols)\n",
        "        '''\n",
        "\n",
        "        # Don't hardcode dimensions (except for maybe channels = 3), use the variables in __init__.\n",
        "        # You shouldn't need to add anything else to __init__, all of the embeddings,\n",
        "        # dropout etc. are already initialized for you.\n",
        "\n",
        "        # Put the preprocessed patches in variable \"out\" with shape (batch_size, length, embed_dim).\n",
        "\n",
        "        # HINT: You can make image patches with .reshape\n",
        "        # e.g.\n",
        "        # x = torch.ones((100, 100))\n",
        "        # x_patches = x.reshape(4, 25, 4, 25)\n",
        "        # where you have 4 * 4 patches with each patch being 25 by 25\n",
        "\n",
        "        h = w = self.image_dim // self.patch_dim\n",
        "        N = images.size(0)\n",
        "        images = images.reshape(N, 3, h, self.patch_dim, w, self.patch_dim)\n",
        "        images = torch.einsum(\"nchpwq -> nhwpqc\", images)\n",
        "        patches = images.reshape(N, h * w, self.input_dim) # (batch, num_patches_per_image, patch_size_unrolled)\n",
        "\n",
        "        patch_embeddings = # TODO: Pass through our patch embedding layer\n",
        "        patch_embeddings = torch.cat([torch.tile(self.cls_token, (N, 1, 1)), patch_embeddings], dim=1)\n",
        "        out = patch_embeddings + torch.tile(self.position_embedding, (N, 1, 1)) # We add positional embeddings to our tokens (not concatenated)\n",
        "        out = # TODO: Pass through our embedding dropout layer\n",
        "\n",
        "        # add padding s.t. input length is multiple of num_heads\n",
        "        add_len = (self.num_heads - out.shape[1]) % self.num_heads\n",
        "        out = torch.cat([out, torch.zeros(N, add_len, out.shape[2], device=device)], dim=1)\n",
        "\n",
        "        # TODO: Pass through each one of our encoder layers\n",
        "\n",
        "        # Pop off and read our classification token we added, see what the value is\n",
        "        cls_head = self.layernorm(torch.squeeze(out[:, 0], dim=1))\n",
        "        logits = self.mlp_head(cls_head)\n",
        "        return logits\n",
        "\n",
        "def get_vit_tiny(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=3,\n",
        "               embed_dim=192, mlp_hidden_dim=768, num_classes=num_classes, dropout=0.1)\n",
        "\n",
        "def get_vit_small(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=6,\n",
        "               embed_dim=384, mlp_hidden_dim=1536, num_classes=num_classes, dropout=0.1)\n",
        "\n",
        "def get_vit_base(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=12,\n",
        "               embed_dim=768, mlp_hidden_dim=3072, num_classes=num_classes, dropout=0.1)"
      ],
      "metadata": {
        "id": "TKbFJj447myz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train the model! You don't need to write any code for this - just run the cell.\n",
        "\n",
        "Remember to change the device variable (in the cell at the beginning of the notebook) to 'cuda' and change your runtime to GPU (Runtime > Change runtime type) as well. For reference, each epoch in the staff solution takes ~3 minutes (so training for 30 epochs will take ~1.5 hours on the Colab GPU; we know this is a long training session)\n",
        "\n",
        "Try to get 65%+ accuracy after 30 epochs."
      ],
      "metadata": {
        "id": "ZJwzQU0qMt9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "data_root = './data/cifar10'\n",
        "train_size = 40000\n",
        "val_size = 10000\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "transform_train = T.Compose([\n",
        "    T.Resize(40),\n",
        "    T.RandomCrop(32),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.95, 1.05)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_val = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=data_root,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train,\n",
        ")\n",
        "\n",
        "val_dataset = datasets.CIFAR10(\n",
        "    root=data_root,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_val,\n",
        ")\n",
        "\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          sampler=sampler.SubsetRandomSampler(range(train_size)))\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                        sampler=sampler.SubsetRandomSampler(range(train_size, 50000)))\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "vit = get_vit_small().to(device)\n",
        "\n",
        "learning_rate = 5e-4 * batch_size / 256\n",
        "num_epochs = 30\n",
        "weight_decay = 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(vit.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=weight_decay)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    train_total = 0\n",
        "    vit.train()\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        \"\"\"TODO:\n",
        "        1. Set inputs and labels to be on device\n",
        "        2. zero out our gradients\n",
        "        3. pass our inputs through the ViT\n",
        "        4. pass our outputs / labels into our loss / criterion\n",
        "        5. backpropagate\n",
        "        6. step our optimizeer\n",
        "        \"\"\"\n",
        "\n",
        "        loss = # TODO\n",
        "\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "        train_acc += torch.sum((torch.argmax(outputs, dim=1) == labels)).item()\n",
        "        train_total += inputs.shape[0]\n",
        "    train_loss = train_loss / train_total\n",
        "    train_acc = train_acc / train_total\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_total = 0\n",
        "    vit.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "\n",
        "            val_loss += loss.item() * inputs.shape[0]\n",
        "            val_acc += torch.sum((torch.argmax(outputs, dim=1) == labels)).item()\n",
        "            val_total += inputs.shape[0]\n",
        "    val_loss = val_loss / val_total\n",
        "    val_acc = val_acc / val_total\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    val_accuracies.append(val_acc)\n",
        "    if val_acc >= max(val_accuracies):\n",
        "        torch.save(vit.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f'[{epoch + 1:2d}] train loss: {train_loss:.3f} | train accuracy: {train_acc:.3f} | val loss: {val_loss:.3f} | val accuracy: {val_acc:.3f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "fkN5vhqp9eI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autograder and Submission"
      ],
      "metadata": {
        "id": "xzbwWHtoWGjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you feel confident that you have a decent model, run the cell below.\n",
        "\n",
        "Feel free to read the code block but **PLEASE DO NOT TOUCH IT**: this will produce a pickle file that will contain your model's predictions on the CIFAR-10 validation set --- tampering with the code block below might mess up the file that you will submit to the Gradescope autograder."
      ],
      "metadata": {
        "id": "DzmBE5NjWOUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "cifar_test = datasets.CIFAR10('./data/cifar10_test', download = True, train = False, transform = transform_val)\n",
        "loader_test = DataLoader(cifar_test, batch_size=32, shuffle=False)\n",
        "\n",
        "vit.load_state_dict(torch.load('best_model.pth'))\n",
        "vit.eval()  # set model to evaluation mode\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for x, _ in loader_test:\n",
        "        x = x.to(device=device)  # move to device, e.g. GPU\n",
        "        scores = vit(x)\n",
        "        _, preds = scores.max(1)\n",
        "        predictions.append(preds)\n",
        "predictions = torch.cat(predictions).tolist()\n",
        "with open(\"my_predictions.pickle\", \"wb\") as file:\n",
        "    pickle.dump(predictions, file)"
      ],
      "metadata": {
        "id": "Xck7BXi9WHsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}