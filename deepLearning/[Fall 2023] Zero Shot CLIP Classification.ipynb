{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanhao5103233729/github-colab/blob/main/%5BFall%202023%5D%20Zero%20Shot%20CLIP%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Classification"
      ],
      "metadata": {
        "id": "4vzW-xxJMbEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll be using CLIP for zero-shot classification on the CIFAR-10 dataset. Zero-shot classification is a technique where a model classifies data that it's never seen examples of during training. As such, it must rely on the quality of the classifications and its ability to generalize."
      ],
      "metadata": {
        "id": "TXqKPuwdMvqq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "Make sure you're running this notebook with the GPU runtime:\n",
        "- Select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "a045066e-1a57-4252-b26a-8cd2e0e82331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-civs0r_e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-civs0r_e\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install tqdm git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import clip\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "Download and instantiate a CLIP model using the `clip` module that we just installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cboKZocQlSYX"
      },
      "outputs": [],
      "source": [
        "# Load in a ViT CLIP model\n",
        "model, preprocess = clip.load(\"ViT-L/14\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Cifar-10 Dataset"
      ],
      "metadata": {
        "id": "KyJFNTN9rKzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sMBlsyRv967",
        "outputId": "41c101b9-99b8-4923-b853-1a3bd4921ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load in CIFAR-10 dataset\n",
        "cifar10 = CIFAR10(root=os.path.expanduser(\"~/.cache\"), download=True, train=False, transform=preprocess)\n",
        "loader = torch.utils.data.DataLoader(cifar10, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines the CIFAR-10 templates and classes, to capture possible language augmentations for image descriptions."
      ],
      "metadata": {
        "id": "jyOxZY_POYP6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zczgkbGrv967"
      },
      "outputs": [],
      "source": [
        "cifar10_templates = [\n",
        "    'a photo of a {}.',\n",
        "    'a blurry photo of a {}.',\n",
        "    'a black and white photo of a {}.',\n",
        "    'a low contrast photo of a {}.',\n",
        "    'a high contrast photo of a {}.',\n",
        "    'a bad photo of a {}.',\n",
        "    'a good photo of a {}.',\n",
        "    'a photo of a small {}.',\n",
        "    'a photo of a big {}.',\n",
        "    'a photo of the {}.',\n",
        "    'a blurry photo of the {}.',\n",
        "    'a black and white photo of the {}.',\n",
        "    'a low contrast photo of the {}.',\n",
        "    'a high contrast photo of the {}.',\n",
        "    'a bad photo of the {}.',\n",
        "    'a good photo of the {}.',\n",
        "    'a photo of the small {}.',\n",
        "    'a photo of the big {}.',\n",
        "]\n",
        "cifar10_classes = [\n",
        "    'airplane',\n",
        "    'automobile',\n",
        "    'bird',\n",
        "    'cat',\n",
        "    'deer',\n",
        "    'dog',\n",
        "    'frog',\n",
        "    'horse',\n",
        "    'ship',\n",
        "    'truck',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz6D-F-Wbrtp"
      },
      "source": [
        "# Creating zero-shot classifier weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to generate zero-shot classification weights for the list of CIFAR-10 classes using text templates and the CLIP model. These weights can be used for zero-shot classification tasks where you want to classify images or other data into one of the specified classes, even if the model hasnâ€™t been explicitly trained on those classes.\n",
        "\n",
        "\n",
        "1. In the zeroshot_classifier function, the code iterates through each class name in the provided class_names list (e.g., 'airplane,' 'automobile,' etc.)\n",
        "2. For each class name, it uses the templates to create formatted textual descriptions.\n",
        "3. These formatted texts are tokenized and converted to embeddings using the CLIP model's text encoder, then normalized to ensure consistent scale.\n",
        "4. The embeddings for each description are averaged to create a representative embedding for the class, and this average embedding is again normalized.\n",
        "5. The code collects all these embeddings and stacks them into a tensor, resulting in a matrix where each column corresponds to the averaged, normalized embeddings for a specific class.\n"
      ],
      "metadata": {
        "id": "9YqdlrOIPYR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRqDoz1Gbsii"
      },
      "outputs": [],
      "source": [
        "def zeroshot_classifier(class_names, templates):\n",
        "    \"\"\"\n",
        "    Generates zero-shot classification weights by averaging the embedded templates of class names.\n",
        "\n",
        "    Parameters:\n",
        "    class_names (list): A list of names of the classes.\n",
        "    templates (list): A list of templates to be used in the formatting of class names.\n",
        "\n",
        "    Returns:\n",
        "    Tensor: A tensor containing the zero-shot classification weights.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        classification_weights = []\n",
        "        for class_name in class_names:\n",
        "            formatted_texts = [template.format(class_name) for template in templates]\n",
        "            tokenized_texts = clip.tokenize(formatted_texts).cuda()\n",
        "\n",
        "            # Get embeddings using the CLIP model's text encoder and normalize\n",
        "            class_embeddings = model.encode_text(tokenized_texts)\n",
        "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Average the embeddings and normalize again\n",
        "            average_embedding = class_embeddings.mean(dim=0)\n",
        "            average_embedding /= average_embedding.norm()\n",
        "\n",
        "            # Store the averaged and normalized embedding\n",
        "            classification_weights.append(average_embedding)\n",
        "\n",
        "        classification_weights = torch.stack(classification_weights, dim=1).cuda()\n",
        "\n",
        "    return classification_weights\n",
        "\n",
        "zeroshot_weights = zeroshot_classifier(cifar10_classes, cifar10_templates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fZo7hG8iJP5"
      },
      "source": [
        "# Zero-shot prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weâ€™re now going to calculate the accuracy of a model's predictions, specifically for the top-k predictions. It's common to measure the accuracy of not just the top-1 prediction (the most confident prediction) but also the top-k predictions (the k most confident predictions), to see if the model is generally leaning toward predicting the right class, just maybe not with the highest certainty.\n",
        "\n",
        "Hereâ€™s how it works:\n",
        "* Identifies the top-k predictions by selecting the indices of the k highest values in the predictions tensor.\n",
        "* Compares these top-k predictions with the ground truth labels to check if they are correct. The results are stored in the correct_comparisons tensor.\n",
        "* For each topk, it calculates the number of correct predictions in the top-k predictions.\n",
        "* The number of correct predictions for each k is returned\n"
      ],
      "metadata": {
        "id": "AWqEDApnQve9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4kPSZoShQxN"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions, targets, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of the top k predictions\n",
        "\n",
        "    Parameters:\n",
        "    predictions (Tensor): The model output.\n",
        "    targets (Tensor): Ground truth labels.\n",
        "    topk (tuple, optional): Tuple indicating the top k predictions to consider for accuracy.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of accuracies for each k in topk.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the top k predictions and their indices\n",
        "    top_k_preds_indices = predictions.topk(max(topk), dim=1, largest=True, sorted=True)[1].t()\n",
        "\n",
        "    # Compare the top k predictions with the targets to see if they are correct\n",
        "    correct_comparisons = top_k_preds_indices.eq(targets.view(1, -1).expand_as(top_k_preds_indices))\n",
        "\n",
        "    accuracies = []\n",
        "    for k in topk:\n",
        "        # Get the number of correct predictions in the top k\n",
        "        correct_in_top_k = correct_comparisons[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "\n",
        "        # Move the correct count to cpu, convert to numpy and then to native Python float\n",
        "        accuracies.append(float(correct_in_top_k.cpu().numpy()))\n",
        "\n",
        "    return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(images, actual_labels, predicted_labels, class_names, num_samples=1):\n",
        "    mu, sigma = torch.tensor((0.48145466, 0.4578275, 0.40821073)), torch.tensor((0.26862954, 0.26130258, 0.27577711))\n",
        "    mu_, sigma_ = -mu / sigma, 1 / sigma\n",
        "    images = transforms.Normalize(mu_, sigma_)(images)\n",
        "    for i in range(num_samples):\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        plt.imshow(np.transpose(images[i].cpu().numpy(), (1, 2, 0)))\n",
        "        plt.title(f\"Actual: {class_names[actual_labels[i]]}\\nPredicted: {class_names[predicted_labels[i]]}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "IxwOF0njc_ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function iterates through batches of data from the DataLoader.\n",
        "\n",
        "For each batch we do the following:\n",
        "\n",
        "* The image data in the batch is moved to the GPU for processing, and the corresponding true labels are also moved to the GPU.\n",
        "* Image features are extracted from the model by encoding the image batch and normalizing them.\n",
        "* Logits are calculated based on the image features and the provided zeroshot_weights.\n",
        "* The top-1 prediction (the class with the highest probability) is determined from the logits.\n",
        "* Call visualize_predictions to display the actual image, its true label, and the predicted label.\n",
        "* Calculate top-1 and top-5 accuracies.\n",
        "\n",
        "After processing all batches, the function calculates the overall top-1 and top-5 accuracies by dividing the accumulated top-1 and top-5 correct predictions by the total number of samples.\n"
      ],
      "metadata": {
        "id": "ZwsxKnn2SKu9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKJ7YsdlkDXo"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_visualize_model(model, loader, zeroshot_weights, visualize_predictions, accuracy, cifar10_classes, max_visualizations=10):\n",
        "    \"\"\"\n",
        "    Evaluate and visualize the model's predictions.\n",
        "\n",
        "    Parameters:\n",
        "    - model: the model to be evaluated\n",
        "    - loader: DataLoader for the evaluation dataset\n",
        "    - zeroshot_weights: the weights used in zero-shot classification\n",
        "    - visualize_predictions: function to visualize the model's predictions\n",
        "    - accuracy: function to calculate the accuracy of the model's predictions\n",
        "    - cifar10_classes: list of class names in CIFAR-10\n",
        "    - max_visualizations: maximum number of predictions to visualize\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        top1_accuracy, top5_accuracy, total_samples = 0., 0., 0.\n",
        "        visualization_count = 0\n",
        "\n",
        "        for batch_index, (image_batch, true_labels) in enumerate(loader):\n",
        "            image_batch, true_labels = image_batch.cuda(), true_labels.cuda()\n",
        "\n",
        "            # Generate image features and normalize them\n",
        "            image_features = model.encode_image(image_batch)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            logits = 100. * image_features @ zeroshot_weights\n",
        "\n",
        "            _, top_prediction = logits.topk(1, dim=1)\n",
        "\n",
        "            predictions.append(top_prediction.cpu())\n",
        "\n",
        "            if visualization_count < max_visualizations:\n",
        "                visualize_predictions(image_batch, true_labels.cpu(), top_prediction.cpu().squeeze(), cifar10_classes)\n",
        "                visualization_count += 1\n",
        "\n",
        "            # Measure and accumulate accuracy\n",
        "            batch_top1_acc, batch_top5_acc = accuracy(logits, true_labels, topk=(1,5))\n",
        "            top1_accuracy += batch_top1_acc\n",
        "            top5_accuracy += batch_top5_acc\n",
        "            total_samples += image_batch.size(0)\n",
        "\n",
        "        # Calculate the overall top-1 and top-5 accuracy\n",
        "        overall_top1_accuracy = (top1_accuracy / total_samples) * 100\n",
        "        overall_top5_accuracy = (top5_accuracy / total_samples) * 100\n",
        "\n",
        "        print(f\"Top-1 accuracy: {overall_top1_accuracy:.2f}\")\n",
        "        print(f\"Top-5 accuracy: {overall_top5_accuracy:.2f}\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "predictions = evaluate_and_visualize_model(model, loader, zeroshot_weights, visualize_predictions, accuracy, cifar10_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO-uVGrd3Ve4"
      },
      "outputs": [],
      "source": [
        "predictions = torch.cat(predictions).tolist()\n",
        "import pickle\n",
        "with open(\"zero_shot_predictions.pickle\", \"wb\") as file:\n",
        "    pickle.dump(predictions, file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "658294e3aa175f6d59f7a2879e95930bec429644047dd9952d1080087cadf91e"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('irispy': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}